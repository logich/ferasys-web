name: Deploy Feral Systems Website

on:
  push:
    branches: [ main, 'claude/**' ]
    paths-ignore:
      - 'README.md'
      - '.gitignore'
      - 'deploy.sh.template'
      - 'deploy.sh'
  pull_request:
    types: [ opened, synchronize, reopened ]
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment:
      name: ${{ github.ref == 'refs/heads/main' && 'production' || 'preview' }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-east-1

    - name: Set deployment environment
      id: env
      run: |
        if [ "${{ github.ref }}" == "refs/heads/main" ]; then
          echo "S3_PATH=s3://${{ secrets.S3_BUCKET }}" >> $GITHUB_OUTPUT
          echo "CLOUDFRONT_ID=${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}" >> $GITHUB_OUTPUT
          echo "ENV_TYPE=production" >> $GITHUB_OUTPUT
          echo "DOMAIN=${{ secrets.DOMAIN || 'ferasys.com' }}" >> $GITHUB_OUTPUT
        else
          # Create branch-specific preview path
          BRANCH_NAME=$(echo ${{ github.ref }} | sed 's/refs\/heads\///' | sed 's/\//-/g')
          echo "S3_PATH=s3://${{ secrets.S3_BUCKET }}/preview/${BRANCH_NAME}" >> $GITHUB_OUTPUT
          echo "CLOUDFRONT_ID=${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}" >> $GITHUB_OUTPUT
          echo "ENV_TYPE=preview" >> $GITHUB_OUTPUT
          echo "BRANCH_NAME=${BRANCH_NAME}" >> $GITHUB_OUTPUT
          DOMAIN="${{ secrets.DOMAIN || 'ferasys.com' }}"
          echo "PREVIEW_URL=https://${DOMAIN}/preview/${BRANCH_NAME}/index.html" >> $GITHUB_OUTPUT
        fi
    
    - name: Check for large files
      run: |
        echo "üîç Checking for large files..."
        large_files=$(find . -type f -size +50M -not -path "./.git/*" -not -path "./node_modules/*")
        if [ -n "$large_files" ]; then
          echo "‚ùå Large files detected (>50MB):"
          echo "$large_files"
          echo "Please remove large files or use Git LFS"
          exit 1
        else
          echo "‚úÖ No large files detected"
        fi

    - name: Compress and optimize files
      run: |
        echo "üì¶ Compressing and optimizing files..."

        # Install compression tools
        sudo apt-get update && sudo apt-get install -y brotli gzip

        # Compress CSS with gzip and brotli
        gzip -9 -k styles.css
        brotli -9 -k styles.css

        # Compress HTML
        gzip -9 -k index.html
        brotli -9 -k index.html

        # Show compression results
        echo "Original sizes:"
        ls -lh index.html styles.css
        echo "Compressed sizes:"
        ls -lh index.html.gz styles.css.gz index.html.br styles.css.br

        echo "‚úÖ Files compressed successfully"

    - name: Sync to S3
      run: |
        echo "üê∫ Deploying to ${{ steps.env.outputs.ENV_TYPE }}..."

        # Sync images and fonts first (with long cache)
        aws s3 cp ferasys.png ${{ steps.env.outputs.S3_PATH }}/ferasys.png \
            --content-type "image/png" \
            --cache-control "public, max-age=31536000, immutable" \
            --acl public-read

        aws s3 cp wolf.jpg ${{ steps.env.outputs.S3_PATH }}/wolf.jpg \
            --content-type "image/jpeg" \
            --cache-control "public, max-age=31536000, immutable" \
            --acl public-read

        aws s3 cp wolf.webp ${{ steps.env.outputs.S3_PATH }}/wolf.webp \
            --content-type "image/webp" \
            --cache-control "public, max-age=31536000, immutable" \
            --acl public-read

        # Upload fonts with long cache
        if [ -d "fonts" ]; then
          aws s3 sync fonts/ ${{ steps.env.outputs.S3_PATH }}/fonts/ \
              --content-type "font/woff2" \
              --cache-control "public, max-age=31536000, immutable" \
              --acl public-read \
              --exclude "*" \
              --include "*.woff2"
        fi

        # Upload compressed CSS (gzip version)
        aws s3 cp styles.css.gz ${{ steps.env.outputs.S3_PATH }}/styles.css \
            --content-type "text/css" \
            --content-encoding "gzip" \
            --cache-control "public, max-age=31536000, immutable" \
            --acl public-read \
            --metadata-directive REPLACE

        # Upload brotli compressed CSS
        aws s3 cp styles.css.br ${{ steps.env.outputs.S3_PATH }}/styles.css.br \
            --content-type "text/css" \
            --content-encoding "br" \
            --cache-control "public, max-age=31536000, immutable" \
            --acl public-read \
            --metadata-directive REPLACE

        # Upload compressed HTML (gzip version with short cache)
        aws s3 cp index.html.gz ${{ steps.env.outputs.S3_PATH }}/index.html \
            --content-type "text/html; charset=utf-8" \
            --content-encoding "gzip" \
            --cache-control "public, max-age=300, must-revalidate" \
            --acl public-read \
            --metadata-directive REPLACE

        # Upload brotli compressed HTML
        aws s3 cp index.html.br ${{ steps.env.outputs.S3_PATH }}/index.html.br \
            --content-type "text/html; charset=utf-8" \
            --content-encoding "br" \
            --cache-control "public, max-age=300, must-revalidate" \
            --acl public-read \
            --metadata-directive REPLACE

        # Upload uncompressed versions as fallback
        aws s3 cp index.html ${{ steps.env.outputs.S3_PATH }}/index-uncompressed.html \
            --content-type "text/html; charset=utf-8" \
            --cache-control "public, max-age=300, must-revalidate" \
            --acl public-read

        aws s3 cp styles.css ${{ steps.env.outputs.S3_PATH }}/styles-uncompressed.css \
            --content-type "text/css" \
            --cache-control "public, max-age=31536000, immutable" \
            --acl public-read

        echo "‚úÖ Files synced to S3 successfully!"
    
    - name: Invalidate CloudFront (production only)
      if: github.ref == 'refs/heads/main'
      run: |
        echo "üîÑ Invalidating CloudFront cache for production..."

        # Only invalidate specific paths that changed (more efficient)
        INVALIDATION_ID=$(aws cloudfront create-invalidation \
            --distribution-id ${{ steps.env.outputs.CLOUDFRONT_ID }} \
            --paths "/index.html" "/index.html.br" "/styles.css" "/styles.css.br" \
            --query 'Invalidation.Id' \
            --output text)

        echo "‚úÖ CloudFront invalidation created: $INVALIDATION_ID"
        echo "‚è≥ Cache invalidation typically completes in 2-5 minutes."

    - name: Comment on PR with preview URL
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `üê∫ **Preview Deployment Ready!**

            üîó **Preview URL:** ${{ steps.env.outputs.PREVIEW_URL }}

            üì¶ **Deployment Details:**
            - Branch: \`${{ steps.env.outputs.BRANCH_NAME }}\`
            - Commit: \`${{ github.sha }}\`
            - Environment: Preview

            ‚è∞ This preview will be updated automatically with new commits.

            üí° **Performance Optimizations Applied:**
            - ‚úÖ Gzip compression (70-80% size reduction)
            - ‚úÖ Brotli compression (even better compression)
            - ‚úÖ Optimized cache headers
            - ‚úÖ CDN edge caching`
          })

    - name: Deployment summary
      run: |
        echo "üéâ Deployment complete!"
        echo "Environment: ${{ steps.env.outputs.ENV_TYPE }}"
        if [ "${{ steps.env.outputs.ENV_TYPE }}" == "preview" ]; then
          echo "Preview URL: ${{ steps.env.outputs.PREVIEW_URL }}"
        else
          echo "üåê Production website updated at https://${{ steps.env.outputs.DOMAIN }}"
        fi
